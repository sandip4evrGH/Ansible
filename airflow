### Airflow Installation Playbook

```yaml
---
- name: Create Python environment
  ansible.builtin.command:
    cmd: python3 -m venv /path/to/your/env
  args:
    creates: /path/to/your/env

- name: Install Ansible 2.9.27
  ansible.builtin.command:
    cmd: /path/to/your/env/bin/pip install ansible==2.9.27
  args:
    creates: /path/to/your/env/bin/ansible

- name: Copy Airflow configuration file
  ansible.builtin.template:
    src: airflow.cfg.j2
    dest: /path/to/your/airflow.cfg

- name: Copy requirements file
  ansible.builtin.template:
    src: requirements.txt.j2
    dest: /path/to/your/requirements.txt

- name: Install requirements
  ansible.builtin.command:
    cmd: /path/to/your/env/bin/pip install -r /path/to/your/requirements.txt
  args:
    creates: /path/to/your/env/bin/airflow


```
### Manage DAGs Playbook

```yaml
---
- name: Manage Airflow DAGs
  hosts: all
  become: yes
  vars:
    dag_action: "add"  # Options: add, remove, update
    dag_name: "example_dag.py"
    dag_folder: "dev"  # Options: dev, production
    repo_url: "https://your-git-repo-url"
    repo_branch: "master"
    dag_repo_path: "/path/to/dags/repo"
    dag_dest_path: "/path/to/airflow/dags/"

  tasks:
    - name: Clone or update repository (only if adding or updating)
      git:
        repo: "{{ repo_url }}"
        dest: "{{ dag_repo_path }}"
        version: "{{ repo_branch }}"
      when: dag_action in ['add', 'update']

    - name: Add DAG
      copy:
        src: "{{ dag_repo_path }}/{{ dag_folder }}/{{ dag_name }}"
        dest: "{{ dag_dest_path }}/{{ dag_name }}"
      when: dag_action == 'add'

    - name: Remove DAG
      file:
        path: "{{ dag_dest_path }}/{{ dag_name }}"
        state: absent
      when: dag_action == 'remove'

    - name: Update DAGs
      synchronize:
        src: "{{ dag_repo_path }}/{{ dag_folder }}/"
        dest: "{{ dag_dest_path }}"
        delete: yes
      when: dag_action == 'update'
```

********************************************

- name: Add or update multiple DAGs
  block:
    - name: Check if DAG exists and is identical
      stat:
        path: "{{ dag_dest_path }}/{{ item }}"
      register: dag_file
      loop: "{{ dag_names }}"
    
    - name: Compare DAG content
      command: diff "{{ dag_repo_path }}/{{ dag_folder }}/{{ item }}" "{{ dag_dest_path }}/{{ item }}"
      register: diff_result
      failed_when: diff_result.rc not in [0, 1]
      changed_when: diff_result.rc == 1
      loop: "{{ dag_names }}"
      when: dag_file.results[loop.index0].stat.exists

    - name: Copy new or updated DAG
      copy:
        src: "{{ dag_repo_path }}/{{ dag_folder }}/{{ item }}"
        dest: "{{ dag_dest_path }}/{{ item }}"
      loop: "{{ dag_names }}"
      when: not dag_file.results[loop.index0].stat.exists or diff_result.changed
  when: dag_action in ['add', 'update']

- name: Remove multiple DAGs
  file:
    path: "{{ dag_dest_path }}/{{ item }}"
    state: absent
  loop: "{{ dag_names }}"
  when: dag_action == 'remove'
********************************************


### Commands to Run the Playbooks

1. **Install Airflow**:
```bash
ansible-playbook install_airflow.yaml -e "airflow_version=2.2.5"
```

2. **Add a DAG**:
```bash
ansible-playbook manage_airflow_dags.yaml -e "dag_action=add dag_name=example_dag.py"
```

3. **Remove a DAG**:
```bash
ansible-playbook manage_airflow_dags.yaml -e "dag_action=remove dag_name=example_dag.py"
```

4. **Update DAGs**:
```bash
ansible-playbook manage_airflow_dags.yaml -e "dag_action=update"
```







To use HashiCorp Vault for managing Kafka user credentials (username and password) securely, you can integrate Vault into your Kafka setup. Here's how you can achieve this:

---

### **1. Set Up Vault**
- **Install Vault**: Follow the [Vault installation guide](https://developer.hashicorp.com/validated-patterns/vault/vault-securing-kafka) to set up Vault on your system.
- **Enable the KV Secrets Engine**: Use the KV (Key-Value) secrets engine to store Kafka credentials.
  ```bash
  vault secrets enable -path=kafka kv
  ```

- **Store Kafka Credentials**:
  For each user, store their credentials in Vault:
  ```bash
  vault kv put kafka/user1 username="user1" password="user1-password"
  vault kv put kafka/user2 username="user2" password="user2-password"
  ```

---

### **2. Configure Kafka to Use Vault**
- **Create a Script to Fetch Credentials**:
  Write a script to fetch credentials from Vault and generate the JAAS configuration dynamically. For example:
  ```bash
  #!/bin/bash
  USERNAME=$(vault kv get -field=username kafka/user1)
  PASSWORD=$(vault kv get -field=password kafka/user1)

  cat <<EOF > kafka_jaas.conf
  KafkaClient {
      org.apache.kafka.common.security.plain.PlainLoginModule required
      username="$USERNAME"
      password="$PASSWORD";
  };
  EOF
  ```

- **Run the Script**:
  Execute the script before starting Kafka to ensure the JAAS file is up-to-date.

---

### **3. Update Kafka Configuration**
- **Broker Configuration**:
  Update the `server.properties` file to use SASL/PLAIN:
  ```plaintext
  listeners=SASL_PLAINTEXT://:9092
  sasl.enabled.mechanisms=PLAIN
  sasl.mechanism.inter.broker.protocol=PLAIN
  inter.broker.listener.name=SASL_PLAINTEXT
  ```

- **Client Configuration**:
  Use the dynamically generated JAAS file in your client properties:
  ```plaintext
  security.protocol=SASL_PLAINTEXT
  sasl.mechanism=PLAIN
  sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
      username="$(vault kv get -field=username kafka/user1)" \
      password="$(vault kv get -field=password kafka/user1)";
  ```

---

### **4. Test the Configuration**
- **List Topics**:
  ```bash
  bin/kafka-topics.sh --bootstrap-server localhost:9092 --list --command-config client.properties
  ```

- **Produce and Consume Messages**:
  Use the dynamically fetched credentials to produce and consume messages.

---

### **5. Automate Credential Rotation**
Vault allows you to rotate secrets periodically. You can set up a policy to rotate Kafka credentials and update them in Vault. Ensure your script fetches the latest credentials before starting Kafka.

---

This approach ensures that Kafka credentials are securely managed and dynamically fetched from Vault, reducing the risk of hardcoding sensitive information. Let me know if you'd like further assistance!

