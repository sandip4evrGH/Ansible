# dags/my_dag.py
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id='my_simple_dag',
    start_date=datetime(2023, 1, 1),
    schedule_interval=None,
    catchup=False
) as dag:
    start_task = BashOperator(
        task_id='start',
        bash_command='echo "Starting..."'
    )
    end_task = BashOperator(
        task_id='end',
        bash_command='echo "Ending..."'
    )
    start_task >> end_task

-----------------------------------------
# dags/bash_output_dag.py
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago

with DAG(
    dag_id='bash_output_passing_example',
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=['bash', 'output_passing'],
) as dag:
    # Task 1: Generate a message and save it to a temporary file
    # We use mktemp to create a unique temporary file path
    generate_message = BashOperator(
        task_id='generate_message_task',
        bash_command="""
            MESSAGE="Hello from the first Bash task!"
            TEMP_FILE=$(mktemp)
            echo "$MESSAGE" > "$TEMP_FILE"
            echo "$TEMP_FILE"
        """,
        do_xcom_push=True, # This pushes the last line of stdout (the temp file path) to XCom
    )

    # Task 2: Read the message from the temporary file and print it
    # We retrieve the XCom value from the previous task using Jinja templating
    read_and_print_message = BashOperator(
        task_id='read_and_print_message_task',
        bash_command="""
            TEMP_FILE="{{ ti.xcom_pull(task_ids='generate_message_task') }}"
            READ_MESSAGE=$(cat "$TEMP_FILE")
            echo "The message from the first task is: $READ_MESSAGE"
            rm "$TEMP_FILE" # Clean up the temporary file
        """,
    )

    # Define the task dependency
    generate_message >> read_and_print_message


-----------------------------------------
# dags/python_output_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago

def _generate_data():
    """
    This function generates a simple string message.
    Its return value will automatically be pushed to XCom.
    """
    message = "Data generated from the first Python task!"
    print(f"Generating data: '{message}'")
    return message

def _process_data(input_message):
    """
    This function receives an input message and processes it (in this case, just prints).
    """
    print(f"Processing data: '{input_message}'")
    processed_message = f"Processed: {input_message.upper()}"
    print(f"Processed data: '{processed_message}'")
    return processed_message # This return value could also be used by a downstream task

with DAG(
    dag_id='python_output_passing_example',
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=['python', 'output_passing'],
) as dag:
    # Task 1: Generate data using a Python callable
    generate_data_task = PythonOperator(
        task_id='generate_data_task',
        python_callable=_generate_data,
    )

    # Task 2: Process the data generated by the first task
    # The 'op_kwargs' argument allows us to pass arguments to the python_callable.
    # The '{{ ti.xcom_pull(task_ids="generate_data_task") }}' pulls the XCom value
    # returned by the 'generate_data_task' and passes it as 'input_message'.
    process_data_task = PythonOperator(
        task_id='process_data_task',
        python_callable=_process_data,
        op_kwargs={
            'input_message': "{{ ti.xcom_pull(task_ids='generate_data_task') }}"
        },
    )

    # Define the task dependency
    generate_data_task >> process_data_task


------------------------------------------------------
# dags/complex_xcom_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
import json

def _fetch_user_data(user_id):
    """
    Simulates fetching user data from an external system.
    Returns a dictionary which Airflow automatically serializes to JSON for XCom.
    """
    print(f"Fetching data for user_id: {user_id}")
    user_data = {
        'id': user_id,
        'name': 'Jane Doe',
        'email': f'jane.doe_{user_id}@example.com',
        'status': 'active'
    }
    print(f"Fetched data: {user_data}")
    return user_data

def _process_user_data(ti):
    """
    Processes the fetched user data.
    'ti' (TaskInstance) is passed implicitly when used in a PythonOperator.
    We explicitly pull XComs using the ti object.
    """
    # Pull the entire dictionary (which was JSON-serialized by XCom)
    fetched_data = ti.xcom_pull(task_ids='fetch_user_data_task')
    
    if not fetched_data:
        raise ValueError("No user data fetched!")

    print(f"Processing data: {fetched_data}")
    processed_status = f"PROCESSED_{fetched_data['status'].upper()}"
    processed_name = fetched_data['name'].upper()

    processed_data = {
        'user_id': fetched_data['id'],
        'processed_name': processed_name,
        'email': fetched_data['email'],
        'new_status': processed_status
    }
    print(f"Processed data: {processed_data}")
    return processed_data

def _confirm_processing(ti):
    """
    Confirms the processing by printing the final processed data.
    """
    final_data = ti.xcom_pull(task_ids='process_user_data_task')
    if final_data:
        print(f"Final confirmation - User {final_data['user_id']} with new status: {final_data['new_status']}")
    else:
        print("No final data to confirm.")

with DAG(
    dag_id='complex_xcom_json_example',
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=['xcom', 'json', 'python'],
) as dag:
    
    fetch_data = PythonOperator(
        task_id='fetch_user_data_task',
        python_callable=_fetch_user_data,
        op_kwargs={'user_id': 123}, # Pass a fixed user_id, could be dynamic
    )

    process_data = PythonOperator(
        task_id='process_user_data_task',
        python_callable=_process_user_data,
        # No op_kwargs for process_data, we pull XCom directly in the callable
    )

    log_final_data = PythonOperator(
        task_id='log_final_data_task',
        python_callable=_confirm_processing,
    )

    # Optional: A Bash task to show you can pull Python XComs in Bash as well
    show_processed_status = BashOperator(
        task_id='show_processed_status_bash',
        bash_command="""
            PROCESSED_DATA_JSON="{{ ti.xcom_pull(task_ids='process_user_data_task') }}"
            NEW_STATUS=$(echo "$PROCESSED_DATA_JSON" | python -c "import sys, json; print(json.load(sys.stdin)['new_status'])")
            echo "Bash task confirming new status: $NEW_STATUS"
        """
    )

    # Define task dependencies
    fetch_data >> process_data >> [log_final_data, show_processed_status]

-----------------------------------------

Sure, let's dive into a few more Airflow DAG examples that cover common patterns, including:
 * Chaining Operators with XComs (more complex data)
 * Using a Sensor for External Dependency
 * Branching based on a Condition
Example 1: Chaining Operators with XComs (JSON Data)
This example shows how to pass a more complex data structure (a dictionary, which Airflow serializes to JSON) between Python tasks.
# dags/complex_xcom_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
import json

def _fetch_user_data(user_id):
    """
    Simulates fetching user data from an external system.
    Returns a dictionary which Airflow automatically serializes to JSON for XCom.
    """
    print(f"Fetching data for user_id: {user_id}")
    user_data = {
        'id': user_id,
        'name': 'Jane Doe',
        'email': f'jane.doe_{user_id}@example.com',
        'status': 'active'
    }
    print(f"Fetched data: {user_data}")
    return user_data

def _process_user_data(ti):
    """
    Processes the fetched user data.
    'ti' (TaskInstance) is passed implicitly when used in a PythonOperator.
    We explicitly pull XComs using the ti object.
    """
    # Pull the entire dictionary (which was JSON-serialized by XCom)
    fetched_data = ti.xcom_pull(task_ids='fetch_user_data_task')
    
    if not fetched_data:
        raise ValueError("No user data fetched!")

    print(f"Processing data: {fetched_data}")
    processed_status = f"PROCESSED_{fetched_data['status'].upper()}"
    processed_name = fetched_data['name'].upper()

    processed_data = {
        'user_id': fetched_data['id'],
        'processed_name': processed_name,
        'email': fetched_data['email'],
        'new_status': processed_status
    }
    print(f"Processed data: {processed_data}")
    return processed_data

def _confirm_processing(ti):
    """
    Confirms the processing by printing the final processed data.
    """
    final_data = ti.xcom_pull(task_ids='process_user_data_task')
    if final_data:
        print(f"Final confirmation - User {final_data['user_id']} with new status: {final_data['new_status']}")
    else:
        print("No final data to confirm.")

with DAG(
    dag_id='complex_xcom_json_example',
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=['xcom', 'json', 'python'],
) as dag:
    
    fetch_data = PythonOperator(
        task_id='fetch_user_data_task',
        python_callable=_fetch_user_data,
        op_kwargs={'user_id': 123}, # Pass a fixed user_id, could be dynamic
    )

    process_data = PythonOperator(
        task_id='process_user_data_task',
        python_callable=_process_user_data,
        # No op_kwargs for process_data, we pull XCom directly in the callable
    )

    log_final_data = PythonOperator(
        task_id='log_final_data_task',
        python_callable=_confirm_processing,
    )

    # Optional: A Bash task to show you can pull Python XComs in Bash as well
    show_processed_status = BashOperator(
        task_id='show_processed_status_bash',
        bash_command="""
            PROCESSED_DATA_JSON="{{ ti.xcom_pull(task_ids='process_user_data_task') }}"
            NEW_STATUS=$(echo "$PROCESSED_DATA_JSON" | python -c "import sys, json; print(json.load(sys.stdin)['new_status'])")
            echo "Bash task confirming new status: $NEW_STATUS"
        """
    )

    # Define task dependencies
    fetch_data >> process_data >> [log_final_data, show_processed_status]


Explanation:
 * _fetch_user_data: Returns a dictionary. Airflow automatically serializes this to JSON when pushing to XCom.
 * _process_user_data: Receives the ti (TaskInstance) object. It explicitly uses ti.xcom_pull() to retrieve the entire dictionary (which Airflow deserializes from JSON back to a Python dict). It then processes the data and returns a new dictionary.
 * _confirm_processing: Demonstrates pulling the XCom from the second Python task.
 * show_processed_status (BashOperator): This is a cool part! It shows how to access a JSON XCom value in a BashOperator.
   * "{{ ti.xcom_pull(task_ids='process_user_data_task') }}" retrieves the JSON string.
   * | python -c "import sys, json; print(json.load(sys.stdin)['new_status'])" pipes the JSON string to a mini Python script that parses the JSON from stdin and extracts the new_status field. This is a common pattern for handling JSON in Bash.
Example 2: Using a Sensor for External Dependency
Sensors are a special type of operator that wait for a certain condition to be met. This is crucial for data pipelines that depend on external events (e.g., a file arriving, an API call returning a specific status, a database table having new data).
# dags/sensor_example_dag.py
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.utils.dates import days_ago
from datetime import timedelta

with DAG(
    dag_id='sensor_dependency_example',
    start_date=days_ago(1),
    schedule_interval=timedelta(hours=1), # This DAG runs hourly
    catchup=False,
    tags=['sensor', 'dependency'],
) as dag:
    
    # Task 1: Simulate an upstream DAG (this DAG doesn't exist, it's just for the sensor)
    # In a real scenario, this would be a task in a different DAG
    # For demonstration, we'll imagine 'upstream_data_ingestion_dag' finishes at 10:00 AM
    # and this DAG, running hourly, would wait for the 10:00 AM instance of that DAG.
    start_processing = BashOperator(
        task_id='start_processing_task',
        bash_command='echo "Starting processing after sensor fires..."'
    )

    # Sensor Task: Wait for a specific task in another DAG to complete successfully
    wait_for_upstream_dag = ExternalTaskSensor(
        task_id='wait_for_upstream_data_ready',
        external_dag_id='upstream_data_ingestion_dag', # The DAG ID of the upstream DAG
        external_task_id='ingest_data_task',           # The task ID within the upstream DAG to wait for
        allowed_states=['success'],                    # Only proceed if the external task succeeded
        failed_states=['failed', 'skipped'],           # If it fails, mark this sensor as failed
        mode='poke',                                   # How the sensor checks (poke is default, deferrable is better for long waits)
        poke_interval=60,                              # Check every 60 seconds
        timeout=60 * 60 * 4,                           # Fail after 4 hours if condition not met
        # execution_delta=timedelta(hours=1),          # Optional: Wait for an instance 1 hour *prior* to this DAG's execution date
        # execution_date_fn=lambda dt: dt - timedelta(hours=1) # More flexible way to define the execution date to check
    )

    # Task 3: Continue with your processing after the sensor succeeds
    continue_data_pipeline = BashOperator(
        task_id='continue_data_pipeline_task',
        bash_command='echo "Upstream data is ready. Continuing with pipeline..."'
    )

    # Define dependencies
    start_processing >> wait_for_upstream_dag >> continue_data_pipeline


Explanation:
 * ExternalTaskSensor: This sensor waits for a task in another DAG (external_dag_id) to reach a specific allowed_states.
 * external_dag_id and external_task_id: Crucial for defining which external task to monitor.
 * allowed_states and failed_states: Define what states are considered a success or failure for the sensor.
 * mode='poke': The default mode, where the sensor task continuously occupies a worker slot and "pokes" (checks) the condition. For long waits, mode='reschedule' or mode='deferrable' (if using the deferrable operators and a triggerer) are more efficient as they release the worker slot.
 * poke_interval and timeout: Control how often the sensor checks and when it should give up and fail.
 * execution_delta / execution_date_fn: These are important for aligning the execution dates between the current DAG and the external DAG. If upstream_data_ingestion_dag generates data for its 10 AM run, and this DAG also runs at 10 AM and needs that data, you might need to adjust the execution_date_fn to look for the data corresponding to its own execution date.
To actually run this example: You would need to create a dummy DAG named upstream_data_ingestion_dag with a task named ingest_data_task that sometimes succeeds and sometimes fails to see the sensor's behavior.
Example 3: Branching based on a Condition (BranchPythonOperator)
This example demonstrates how to use the BranchPythonOperator to dynamically determine the next task to execute based on a Python function's return value.
# dags/branching_example_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.python import BranchPythonOperator
from airflow.utils.dates import days_ago

# --- Python Callables for Branching ---
def _determine_path(ti):
    """
    Determines which path to take based on a simulated condition.
    Returns the task_id of the next task to execute.
    """
    # Simulate a condition: e.g., current hour, data volume, external flag
    import random
    if random.choice([True, False]):
        print("Condition met! Taking path_a.")
        ti.xcom_push(key='branch_chosen', value='path_a')
        return 'process_path_a_task'
    else:
        print("Condition NOT met. Taking path_b.")
        ti.xcom_push(key='branch_chosen', value='path_b')
        return 'process_path_b_task'

def _process_path_a():
    print("Executing operations for Path A.")
    return "Path A completed!"

def _process_path_b():
    print("Executing operations for Path B.")
    return "Path B completed!"

def _join_paths(ti):
    """
    This task will run regardless of which path was taken.
    It can optionally check which path was taken via XComs.
    """
    chosen_branch = ti.xcom_pull(task_ids='determine_path_task', key='branch_chosen')
    print(f"Both paths converge here. The chosen branch was: {chosen_branch}")
    # You could also pull results from the chosen path if needed
    if chosen_branch == 'path_a':
        result_a = ti.xcom_pull(task_ids='process_path_a_task')
        print(f"Result from Path A: {result_a}")
    elif chosen_branch == 'path_b':
        result_b = ti.xcom_pull(task_ids='process_path_b_task')
        print(f"Result from Path B: {result_b}")
    print("Joining point reached.")

with DAG(
    dag_id='branching_example',
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=['branching', 'dynamic'],
) as dag:

    start_task = BashOperator(
        task_id='start_task',
        bash_command='echo "Starting branching DAG..."'
    )

    # BranchPythonOperator: determines the next task to run
    determine_path = BranchPythonOperator(
        task_id='determine_path_task',
        python_callable=_determine_path,
    )

    # Tasks for Path A
    process_path_a = PythonOperator(
        task_id='process_path_a_task',
        python_callable=_process_path_a,
    )

    # Tasks for Path B
    process_path_b = PythonOperator(
        task_id='process_path_b_task',
        python_callable=_process_path_b,
    )

    # Dummy tasks to ensure paths are skipped if not chosen
    # Airflow skips tasks that are not returned by the BranchPythonOperator
    # and whose upstream dependencies were skipped.
    skip_a_placeholder = BashOperator(
        task_id='skip_a_placeholder_task',
        bash_command='echo "This task will be skipped if Path B is chosen."',
        trigger_rule='none_failed_min_one_success', # Allows execution if one upstream succeeds and others are skipped
    )

    skip_b_placeholder = BashOperator(
        task_id='skip_b_placeholder_task',
        bash_command='echo "This task will be skipped if Path A is chosen."',
        trigger_rule='none_failed_min_one_success',
    )


    # Joining point: This task will run after either path_a or path_b completes
    join_paths = PythonOperator(
        task_id='join_paths_task',
        python_callable=_join_paths,
        trigger_rule='none_failed_min_one_success', # Crucial for join point
                                                     # It means: run if none of my direct upstream tasks failed,
                                                     # and at least one of them succeeded (or was skipped, which counts as success here)
    )

    # Define dependencies
    start_task >> determine_path

    # If 'determine_path' returns 'process_path_a_task', then this path runs
    determine_path >> process_path_a

    # If 'determine_path' returns 'process_path_b_task', then this path runs
    determine_path >> process_path_b

    # All paths eventually lead to the join_paths_task
    # Note: If a task in a branch is skipped, its downstream tasks are also skipped by default
    # unless a different trigger_rule is specified.
    process_path_a >> join_paths
    process_path_b >> join_paths
    # The dummy tasks are important to ensure all paths are considered by the join_paths_task
    # if you had other downstream tasks that only depend on the dummy_tasks.
    # In this specific case, `join_paths` only depends on the `process_path_a` and `process_path_b`
    # so the placeholders are not strictly needed for the `join_paths` task,
    # but they're good for illustrating the skipped state in the UI.
    determine_path >> skip_a_placeholder # This path will be skipped if `determine_path` returns `process_path_b_task`
    determine_path >> skip_b_placeholder # This path will be skipped if `determine_path` returns `process_path_a_task`
    skip_a_placeholder >> join_paths # For completeness, if you wanted the skipped placeholders to feed into join.
    skip_b_placeholder >> join_paths # However, the direct dependencies are cleaner.


Explanation:
 * BranchPythonOperator: Its python_callable must return the task_id (string) of the task that should be executed next. All other direct downstream tasks from the BranchPythonOperator will be marked as skipped.
 * _determine_path: This function contains the logic to decide which branch to take. It uses random.choice for demonstration, but in a real scenario, this would be based on business logic, data checks, configuration, etc. It also pushes the chosen path to XCom for later logging.
 * trigger_rule='none_failed_min_one_success': This is crucial for the join_paths_task. By default, a task only runs if all its upstream tasks succeed (all_success trigger rule). When using branching, one of the upstream tasks will be skipped. none_failed_min_one_success means the task will run if at least one direct upstream task succeeds, and no direct upstream tasks fail. Skipped tasks are not considered failures.
 * Skipped Tasks: When a branch is not taken, the tasks in that branch (and any downstream from them) will appear as skipped in the Airflow UI.
These examples should give you a solid foundation for building more sophisticated and dynamic Airflow DAGs!
You got it! Let's dive into some more examples of Airflow DAGs, including more advanced XCom usage, a common pattern with Sensors, and a glimpse into Hooks.
Example 3: PythonOperator with Multiple XCom Outputs
This DAG demonstrates how a single Python task can return a dictionary, and each key-value pair in that dictionary can be pushed as a separate XCom. This is very useful when a task computes several distinct pieces of data that downstream tasks might need.
# dags/python_multiple_xcom_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago

def _generate_multiple_outputs():
    """
    This function generates a dictionary with multiple pieces of information.
    By default, Airflow will push each key-value pair as a separate XCom.
    """
    file_path = "/tmp/my_data_file_{{ ds_nodash }}.txt"
    record_count = 12345
    status_code = 200

    with open(file_path, "w") as f:
        f.write(f"This is some sample data for {record_count} records.\n")
        f.write(f"Status: {status_code}\n")

    print(f"Generated file: {file_path}")
    print(f"Record count: {record_count}")
    print(f"Status code: {status_code}")

    # The returned dictionary's keys will become XCom keys
    return {
        'output_file_path': file_path,
        'output_record_count': record_count,
        'output_status_code': status_code
    }

def _consume_multiple_outputs(**kwargs):
    """
    This function consumes the multiple outputs pushed by the previous task.
    """
    ti = kwargs['ti']
    file_path = ti.xcom_pull(task_ids='generate_multiple_outputs_task', key='output_file_path')
    record_count = ti.xcom_pull(task_ids='generate_multiple_outputs_task', key='output_record_count')
    status_code = ti.xcom_pull(task_ids='generate_multiple_outputs_task', key='output_status_code')

    print(f"Received file path: {file_path}")
    print(f"Received record count: {record_count}")
    print(f"Received status code: {status_code}")

    # Read content from the file
    if file_path:
        with open(file_path, "r") as f:
            content = f.read()
            print(f"\nContent of the file:\n{content}")

    # Perform some conditional logic based on the status code
    if status_code == 200:
        print("Data generation was successful!")
    else:
        print(f"Warning: Data generation completed with status code {status_code}")

with DAG(
    dag_id='python_multiple_xcom_outputs',
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=['python', 'xcom', 'multiple_outputs'],
) as dag:
    generate_outputs = PythonOperator(
        task_id='generate_multiple_outputs_task',
        python_callable=_generate_multiple_outputs,
        # In Airflow 2.x, returning a dictionary from a PythonOperator
        # automatically pushes each key as a separate XCom if multiple_outputs=True
        # For @task decorated functions, this is the default behavior.
        # For PythonOperator, this is generally implicitly handled for dictionary returns.
        # If you were using the @task decorator, you'd specify @task(multiple_outputs=True)
    )

    consume_outputs = PythonOperator(
        task_id='consume_multiple_outputs_task',
        python_callable=_consume_multiple_outputs,
        provide_context=True, # Necessary to access 'ti' in kwargs
    )

    generate_outputs >> consume_outputs

Explanation:
 * _generate_multiple_outputs(): This Python callable returns a dictionary. Airflow automatically pushes each key-value pair of this dictionary as a separate XCom entry.
 * _consume_multiple_outputs(**kwargs):
   * provide_context=True is essential for PythonOperator to receive the ti (TaskInstance) object in kwargs, which is used to pull XComs.
   * ti.xcom_pull(task_ids='...', key='...'): We explicitly specify the key of the XCom we want to pull. This allows us to retrieve individual values from the dictionary pushed by the upstream task.
   * The task then demonstrates reading from the generated file and performing conditional logic based on the status_code.
Example 4: Using a Sensor for External File Arrival
Sensors are special operators that wait for a condition to be met before allowing downstream tasks to execute. A common use case is waiting for a file to appear in a specific location.
# dags/file_sensor_dag.py
from airflow import DAG
from airflow.sensors.filesystem import FileSensor
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
import os

# Define the directory where the sensor will look for the file
# For local testing, you can create this directory and file manually
# E.g., /tmp/airflow_data/input_file.txt
MONITORED_DIR = "/tmp/airflow_data"
INPUT_FILE_NAME = "input_file.txt"
FULL_FILE_PATH = os.path.join(MONITORED_DIR, INPUT_FILE_NAME)

with DAG(
    dag_id='file_sensor_example',
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=['sensor', 'filesystem'],
) as dag:
    # Task 1: Wait for the input file to appear
    wait_for_file = FileSensor(
        task_id='wait_for_input_file',
        filepath=FULL_FILE_PATH,
        fs_conn_id='fs_default', # Default filesystem connection
        poke_interval=5, # Check every 5 seconds
        timeout=600, # Fail after 10 minutes if file not found
    )

    # Task 2: Process the file once it appears
    process_file = BashOperator(
        task_id='process_input_file',
        bash_command=f"""
            echo "Input file '{FULL_FILE_PATH}' found!"
            echo "Content of the file:"
            cat "{FULL_FILE_PATH}"
            echo "Processing complete. Deleting file..."
            rm "{FULL_FILE_PATH}"
            echo "File deleted."
        """,
    )

    # Define the task dependency
    wait_for_file >> process_file

To test this DAG:
 * Save the DAG as file_sensor_dag.py in your DAGs folder.
 * Make sure the directory /tmp/airflow_data exists on your Airflow worker. If not, create it: mkdir -p /tmp/airflow_data.
 * Enable the DAG in the Airflow UI and trigger it.
 * Observe the wait_for_input_file task. It will remain in a "running" or "scheduled" state (depending on poke_interval and reschedule mode, default is poke).
 * Now, create the input_file.txt inside /tmp/airflow_data. For example: echo "Hello, Airflow!" > /tmp/airflow_data/input_file.txt
 * Within a few seconds (based on poke_interval), the wait_for_input_file task should succeed, and process_input_file will then run, read the content, and delete the file.
Explanation:
 * FileSensor: This sensor specifically waits for a file at a given filepath.
   * filepath: The full path to the file it's monitoring.
   * fs_conn_id: Airflow uses "connections" to interact with external systems. fs_default is a built-in connection that points to the local filesystem.
   * poke_interval: How often the sensor "pokes" (checks) for the condition (default is 5 seconds).
   * timeout: How long the sensor will wait before failing the task.
 * process_file: This BashOperator only executes after the FileSensor has successfully detected the file. It then performs some actions on the file.
Example 5: Using a Hook directly (PostgresHook)
While operators often encapsulate the use of Hooks, you can use Hooks directly within your PythonOperator tasks. This is useful for more custom logic where a predefined operator might not exist, or when you need fine-grained control over database interactions.
Prerequisites:
 * PostgreSQL Database: You need a running PostgreSQL database. You can use a local one or a Docker container.
 * Airflow Connection: Configure a PostgreSQL connection in Airflow UI.
   * Go to Admin -> Connections.
   * Click + to add a new connection.
   * Conn Id: postgres_default (or any name you choose, but remember it)
   * Conn Type: PostgreSQL
   * Host: localhost (or your DB host)
   * Port: 5432 (default)
   * Schema: your_database_name
   * Login: your_username
   * Password: your_password
# dags/postgres_hook_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.providers.postgres.hooks.postgres import PostgresHook # Import the Hook

def _create_and_insert_data():
    """
    Uses PostgresHook to connect to a PostgreSQL database,
    create a table, and insert data.
    """
    # Instantiate the PostgresHook with the connection ID
    # This conn_id must match the one configured in Airflow UI
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')

    # Define SQL commands
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS my_airflow_data (
        id SERIAL PRIMARY KEY,
        name VARCHAR(255) NOT NULL,
        value INTEGER,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """
    insert_data_sql = """
    INSERT INTO my_airflow_data (name, value) VALUES (%s, %s);
    """

    try:
        # Get a connection object
        conn = pg_hook.get_conn()
        cursor = conn.cursor()

        # Execute create table
        print("Creating table if not exists...")
        cursor.execute(create_table_sql)
        conn.commit()
        print("Table created or already exists.")

        # Execute insert data
        data_to_insert = [
            ("Airflow Example 1", 100),
            ("Airflow Example 2", 200),
            ("Airflow Example 3", 300)
        ]
        print(f"Inserting {len(data_to_insert)} rows...")
        cursor.executemany(insert_data_sql, data_to_insert)
        conn.commit()
        print("Data inserted successfully.")

    except Exception as e:
        print(f"Error during database operation: {e}")
        # Re-raise the exception to mark the task as failed
        raise

    finally:
        if conn:
            conn.close()

def _fetch_and_process_data():
    """
    Uses PostgresHook to fetch data from the database and print it.
    """
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')

    select_data_sql = "SELECT id, name, value FROM my_airflow_data ORDER BY id DESC LIMIT 5;"

    try:
        print("Fetching data from my_airflow_data table...")
        # Use get_records for convenience to fetch all rows
        records = pg_hook.get_records(select_data_sql)

        if records:
            print("Fetched Records:")
            for record in records:
                print(f"ID: {record[0]}, Name: {record[1]}, Value: {record[2]}")
        else:
            print("No records found.")

    except Exception as e:
        print(f"Error during data fetch: {e}")
        raise

with DAG(
    dag_id='postgres_hook_example',
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=['hook', 'database', 'postgres'],
) as dag:
    create_and_insert_task = PythonOperator(
        task_id='create_and_insert_data_task',
        python_callable=_create_and_insert_data,
    )

    fetch_and_process_task = PythonOperator(
        task_id='fetch_and_process_data_task',
        python_callable=_fetch_and_process_data,
    )

    create_and_insert_task >> fetch_and_process_task

Explanation:
 * PostgresHook: This is a specific type of Hook provided by Airflow's apache-airflow-providers-postgres package. It provides methods to interact with PostgreSQL databases.
 * postgres_conn_id='postgres_default': The conn_id parameter tells the Hook which Airflow connection to use for database credentials.
 * pg_hook.get_conn(): This method retrieves a connection object (e.g., a psycopg2 connection) from the configured Airflow connection. You can then use this connection and its cursor to execute SQL commands directly.
 * pg_hook.get_records(sql): This is a convenience method provided by many database Hooks to execute a SELECT query and return all results as a list of tuples.
 * Error Handling: It's good practice to include try...except...finally blocks when interacting with external systems to ensure connections are closed and errors are logged appropriately.
These examples cover more practical scenarios, demonstrating how to handle multiple data points via XComs, wait for external events using Sensors, and directly interact with external systems like databases using Hooks.

----------------------------------------------------

# dags/topology_runner_dag.py
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.sensors.bash import BashSensor
from airflow.utils.dates import days_ago
import json
import logging

# Configure logging for better visibility in task logs
log = logging.getLogger(__name__)

# --- Configuration Variables ---
TOPOLOGY_STATUS_URL = "http://your_topology_status_endpoint.com/status" # REPLACE WITH YOUR ACTUAL URL
TOPOLOGY_RUN_COMMAND = "your_topology_cli_command --config /path/to/config.yaml --name my_topology_run" # REPLACE WITH YOUR ACTUAL COMMAND
S3_BUCKET_NAME = "your-s3-bucket-name" # REPLACE WITH YOUR S3 BUCKET
S3_COPY_COMMAND_TEMPLATE = "aws s3 cp /path/to/local/data/{{ ds_nodash }}/ s3://{{ params.s3_bucket }}/topology_data/{{ ds_nodash }}/ --recursive" # Adjust local path as needed

def _check_topology_status(url):
    """
    Checks if the topology is running by hitting a URL.
    Returns True if running, False otherwise.
    (This is a placeholder; you'll need to parse the actual URL response)
    """
    import requests # Ensure requests is installed in your Airflow environment

    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)

        # IMPORTANT: Adjust this logic based on your actual URL's response
        # Example 1: If the URL returns plain text like "RUNNING" or "IDLE"
        status_text = response.text.strip().upper()
        if "RUNNING" in status_text:
            log.info(f"Topology status: {status_text} (RUNNING)")
            return True
        else:
            log.info(f"Topology status: {status_text} (NOT RUNNING/IDLE)")
            return False

        # Example 2: If the URL returns JSON like {"status": "running"}
        # status_json = response.json()
        # if status_json.get("status") == "running":
        #     log.info(f"Topology status: {status_json.get('status')} (RUNNING)")
        #     return True
        # else:
        #     log.info(f"Topology status: {status_json.get('status')} (NOT RUNNING/IDLE)")
        #     return False

    except requests.exceptions.RequestException as e:
        log.error(f"Error checking topology status: {e}")
        # If there's an error reaching the URL, assume it's not running or unknown,
        # but for safety, you might want to consider this a failure depending on your desired behavior.
        # For this DAG, we'll assume "not running" if status check fails.
        return False
    except Exception as e:
        log.error(f"Unexpected error in _check_topology_status: {e}")
        return False

def _parse_topology_output(**kwargs):
    """
    Parses the output of the 'run_topology_command' task to extract
    topology ID and other info, then pushes them to XComs.
    """
    ti = kwargs['ti']
    run_output = ti.xcom_pull(task_ids='run_topology_command')
    log.info(f"Raw output from run_topology_command:\n{run_output}")

    topology_id = None
    other_info = "N/A"

    # --- IMPORTANT: ADJUST THIS PARSING LOGIC TO YOUR ACTUAL OUTPUT ---
    # Example parsing (assuming output format like: "Topology ID: abc123def, Status: Started")
    if run_output:
        for line in run_output.splitlines():
            if "Topology ID:" in line:
                try:
                    # Regex might be more robust for complex patterns
                    topology_id = line.split("Topology ID:")[1].strip().split(",")[0].strip()
                except IndexError:
                    log.warning("Could not parse topology ID from line.")
            if "Other Info:" in line: # Example for another piece of info
                other_info = line.split("Other Info:")[1].strip()

    if topology_id:
        log.info(f"Parsed Topology ID: {topology_id}")
        ti.xcom_push(key='topology_id', value=topology_id)
    else:
        log.error("Failed to parse Topology ID from run command output!")
        raise ValueError("Topology ID could not be extracted.")

    log.info(f"Parsed Other Info: {other_info}")
    ti.xcom_push(key='topology_other_info', value=other_info)


with DAG(
    dag_id='topology_runner_dag',
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=['topology', 'conditional', 'sensor', 's3'],
    params={
        's3_bucket': S3_BUCKET_NAME
    }
) as dag:
    # Task 1: Check if topology is running
    check_status = PythonOperator(
        task_id='check_topology_running_status',
        python_callable=_check_topology_status,
        op_kwargs={'url': TOPOLOGY_STATUS_URL},
        do_xcom_push=True, # Push the boolean result to XCom
    )

    # Task 2: Conditionally run topology if not running
    run_topology_command = BashOperator(
        task_id='run_topology_command',
        bash_command=TOPOLOGY_RUN_COMMAND,
        do_xcom_push=True, # Push entire stdout to XCom for parsing
        # This task will only run if check_status returns False (topology is NOT running)
        # We achieve this with a branch operator or by setting a specific trigger_rule
        # combined with a downstream task. For this, we'll use a `ShortCircuitOperator` or similar pattern
        # to skip if running. Let's use a Python branching function for clarity.
    )

    def _decide_to_run_topology(**kwargs):
        ti = kwargs['ti']
        is_running = ti.xcom_pull(task_ids='check_topology_running_status')
        if is_running:
            log.info("Topology is already running. Skipping 'run_topology_command' and its downstream tasks.")
            return 'skip_run_topology_tasks' # Name of a dummy task to skip to
        else:
            log.info("Topology is not running. Proceeding to run topology command.")
            return 'run_topology_command' # Name of the actual task to run

    decide_to_run_or_skip = PythonOperator(
        task_id='decide_to_run_or_skip',
        python_callable=_decide_to_run_or_skip,
        provide_context=True,
    )

    # A dummy task to skip to if the topology is already running
    # This ensures the DAG run can still complete successfully without errors.
    skip_run_topology_tasks = BashOperator(
        task_id='skip_run_topology_tasks',
        bash_command='echo "Skipping topology run as it is already active."',
        trigger_rule='all_done', # This dummy task should always succeed if reached
    )

    # Task 3: Get topology ID and other info from output
    parse_topology_output = PythonOperator(
        task_id='parse_topology_output',
        python_callable=_parse_topology_output,
        provide_context=True,
        # This task only runs if 'run_topology_command' actually runs and succeeds.
        trigger_rule='all_success' # Default trigger_rule, just for emphasis
    )

    # Task 4: Run status checker task (BashSensor)
    # This will check if the topology has completed by grepping logs or status output
    check_topology_completion = BashSensor(
        task_id='check_topology_completion',
        # This command should output something that indicates completion or not
        # e.g., tail -n 100 /path/to/topology/log.txt | grep "CAT process completed"
        # The sensor succeeds if the command returns a 0 exit code.
        bash_command=f"{{{{ ti.xcom_pull(task_ids='run_topology_command', key='return_value') or 'echo \"No topology output to check\"' }}}} | grep -q 'CAT process completed'",
        # The above line is a bit tricky: it tries to use the stdout of the run_topology_command
        # as input to grep. If run_topology_command is skipped, its xcom_pull will be None,
        # so we default to 'echo "..."' which will ensure the grep command itself doesn't error out.
        # A more robust approach might be to grep a dedicated log file created by your topology.
        # Example for dedicated log file check:
        # bash_command=f"grep -q 'CAT process completed' /var/log/topology/{{{{ ti.xcom_pull(task_ids='parse_topology_output', key='topology_id') }}}}}.log",


        poke_interval=300, # Check every 5 minutes (300 seconds)
        timeout=7200, # Max wait 2 hours (adjust as needed)
        mode='poke', # 'poke' keeps the worker slot busy; 'reschedule' frees it up.
                     # 'reschedule' is generally better for long waits, but requires
                     # deferrable operators (which BashSensor is, as of Airflow 2.2+)
    )

    # Task 5: Copy data to S3 bucket
    copy_data_to_s3 = BashOperator(
        task_id='copy_data_to_s3',
        bash_command=S3_COPY_COMMAND_TEMPLATE,
        # Ensure AWS CLI is configured and accessible
        # The params.s3_bucket will be resolved from the DAG's params dictionary
    )

    # --- Define Dependencies ---
    # Path 1: Topology is already running, skip the run steps
    check_status >> decide_to_run_or_skip >> skip_run_topology_tasks

    # Path 2: Topology is not running, proceed to run it
    decide_to_run_or_skip >> run_topology_command >> parse_topology_output

    # Both paths converge to the status checker.
    # The `trigger_rule='one_success'` here is crucial:
    # It means this task will run if *either* 'parse_topology_output' (meaning a new run occurred)
    # *OR* 'skip_run_topology_tasks' (meaning it was already running) succeeds.
    [parse_topology_output, skip_run_topology_tasks] >> check_topology_completion

    # Finally, copy to S3 after completion check
    check_topology_completion >> copy_data_to_s3
